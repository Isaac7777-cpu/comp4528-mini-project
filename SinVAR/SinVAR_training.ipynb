{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Overall\n",
    "\n",
    "In order to train the SinVAR, normally, it includes to train the VAE first. However, given the time-horizon for the current project, \n",
    "and to stay with the original VAR paper, I will use the pretrained VQVAE as they have used. However, I will look into fine-tune the VQVAE if possible. \n",
    "\n",
    "Then, after the VQVAE, we have to train the VAR."
   ],
   "id": "e0c9f6b34ef3a645"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T08:28:26.576478Z",
     "start_time": "2025-04-20T08:28:25.439464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple"
   ],
   "id": "cb34bfbaaf8dfe0c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T08:28:26.587017Z",
     "start_time": "2025-04-20T08:28:26.577706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from vqvae import VQVAE\n",
    "from var import VAR\n",
    "\n",
    "\n",
    "def build_vae_var(\n",
    "        # Shared args\n",
    "        device, patch_nums=(1, 2, 3, 4, 5, 6, 8, 10, 13, 16),  # 10 steps by default\n",
    "        # VQVAE args\n",
    "        V=4096, Cvae=32, ch=160, share_quant_resi=4,\n",
    "        # VAR args\n",
    "        depth=16, attn_l2_norm=True,\n",
    "        init_head=0.02, init_std=-1,  # init_std < 0: automated\n",
    ") -> Tuple[VQVAE, VAR]:\n",
    "    heads = depth\n",
    "    width = depth * 64\n",
    "    dpr = 0.1 * depth / 24\n",
    "\n",
    "    # disable built-in initialization for speed\n",
    "    for clz in (nn.Linear, nn.LayerNorm, nn.BatchNorm2d, nn.SyncBatchNorm, nn.Conv1d, nn.Conv2d, nn.ConvTranspose1d,\n",
    "                nn.ConvTranspose2d):\n",
    "        setattr(clz, 'reset_parameters', lambda self: None)\n",
    "\n",
    "    # build models\n",
    "    vae_local = VQVAE(vocab_size=V, z_channels=Cvae, ch=ch, test_mode=True, share_quant_resi=share_quant_resi,\n",
    "                      v_patch_nums=patch_nums).to(device)\n",
    "    var_wo_ddp = VAR(\n",
    "        vae_local=vae_local,\n",
    "        depth=depth, embed_dim=width, num_heads=heads, drop_rate=0., attn_drop_rate=0., drop_path_rate=dpr,\n",
    "        norm_eps=1e-6,\n",
    "        attn_l2_norm=attn_l2_norm,\n",
    "        patch_nums=patch_nums,\n",
    "    ).to(device)\n",
    "    var_wo_ddp.init_weights(init_head=init_head, init_std=init_std)\n",
    "\n",
    "    return vae_local, var_wo_ddp"
   ],
   "id": "7bc5b3a08eff4a57",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## VQVAE\n",
    "\n",
    "- Loading from the Hugging Face\n",
    "- Trying to Reconstruct on Random Crop"
   ],
   "id": "efeab9087ee24c50"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-20T08:28:26.614721Z",
     "start_time": "2025-04-20T08:28:26.587875Z"
    }
   },
   "source": [
    "vae_var_config = {\n",
    "    # Shared\n",
    "    \"device\": torch.device(\"mps\" if torch.mps.is_available() else \"cpu\"),\n",
    "    \"patch_nums\" : (1, 2, 3, 4, 5, 6, 8, 10, 13, 16),\n",
    "\n",
    "    # VAR config (customizable for your setup)\n",
    "    \"depth\": 4,  # VAR transformer depth\n",
    "    \"attn_l2_norm\": True,  \n",
    "\n",
    "    # Initialisation options (irrelevant for non-adaptive setup)\n",
    "    \"init_head\": 0.02,\n",
    "    \"init_std\": -1,  # Use default init scheme\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T08:28:27.868766Z",
     "start_time": "2025-04-20T08:28:26.615633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vae_local, var_wo_ddp = build_vae_var(\n",
    "    V=4096, Cvae=32, ch=160, share_quant_resi=4,  # hard-coded VQVAE hyperparameters\n",
    "    device=vae_var_config['device'],\n",
    "    depth=vae_var_config['depth'], attn_l2_norm=vae_var_config['attn_l2_norm'],\n",
    "    init_head=vae_var_config['init_head'], init_std=vae_var_config['init_std'],\n",
    ")"
   ],
   "id": "c2662b59f48389e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[constructor]  ==== UNCONDITIONAL VAR: using SelfAttnBlock (4 blocks, no class label) ====\n",
      "    [VAR config ] embed_dim=256, num_heads=4, depth=4\n",
      "    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0166667 (tensor([0.0000, 0.0056, 0.0111, 0.0167]))\n",
      "\n",
      "[init_weights] VAR with init_std=0.0360844\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T08:28:28.318023Z",
     "start_time": "2025-04-20T08:28:27.870692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# download checkpoint for VQVAE\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "# Download from Hugging Face\n",
    "hf_home = 'https://huggingface.co/FoundationVision/var/resolve/main'\n",
    "vae_ckpt = 'pretrained-model/vae_ch160v4096z32.pth'\n",
    "if not osp.exists(vae_ckpt): os.system(f'wget {hf_home}/{vae_ckpt}')\n",
    "\n",
    "# load checkpoints\n",
    "vae_local.load_state_dict(torch.load(vae_ckpt, map_location='cpu'), strict=True)"
   ],
   "id": "3f52cd8e8834c301",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VQVAE:\n\tMissing key(s) in state_dict: \"encoder.down.1.block.0.nin_shortcut.weight\", \"encoder.down.1.block.0.nin_shortcut.bias\", \"encoder.down.2.attn.0.norm.weight\", \"encoder.down.2.attn.0.norm.bias\", \"encoder.down.2.attn.0.qkv.weight\", \"encoder.down.2.attn.0.qkv.bias\", \"encoder.down.2.attn.0.proj_out.weight\", \"encoder.down.2.attn.0.proj_out.bias\", \"encoder.down.2.attn.1.norm.weight\", \"encoder.down.2.attn.1.norm.bias\", \"encoder.down.2.attn.1.qkv.weight\", \"encoder.down.2.attn.1.qkv.bias\", \"encoder.down.2.attn.1.proj_out.weight\", \"encoder.down.2.attn.1.proj_out.bias\", \"decoder.up.0.block.0.nin_shortcut.weight\", \"decoder.up.0.block.0.nin_shortcut.bias\", \"decoder.up.2.attn.0.norm.weight\", \"decoder.up.2.attn.0.norm.bias\", \"decoder.up.2.attn.0.qkv.weight\", \"decoder.up.2.attn.0.qkv.bias\", \"decoder.up.2.attn.0.proj_out.weight\", \"decoder.up.2.attn.0.proj_out.bias\", \"decoder.up.2.attn.1.norm.weight\", \"decoder.up.2.attn.1.norm.bias\", \"decoder.up.2.attn.1.qkv.weight\", \"decoder.up.2.attn.1.qkv.bias\", \"decoder.up.2.attn.1.proj_out.weight\", \"decoder.up.2.attn.1.proj_out.bias\", \"decoder.up.2.attn.2.norm.weight\", \"decoder.up.2.attn.2.norm.bias\", \"decoder.up.2.attn.2.qkv.weight\", \"decoder.up.2.attn.2.qkv.bias\", \"decoder.up.2.attn.2.proj_out.weight\", \"decoder.up.2.attn.2.proj_out.bias\". \n\tUnexpected key(s) in state_dict: \"encoder.down.3.block.0.norm1.weight\", \"encoder.down.3.block.0.norm1.bias\", \"encoder.down.3.block.0.conv1.weight\", \"encoder.down.3.block.0.conv1.bias\", \"encoder.down.3.block.0.norm2.weight\", \"encoder.down.3.block.0.norm2.bias\", \"encoder.down.3.block.0.conv2.weight\", \"encoder.down.3.block.0.conv2.bias\", \"encoder.down.3.block.1.norm1.weight\", \"encoder.down.3.block.1.norm1.bias\", \"encoder.down.3.block.1.conv1.weight\", \"encoder.down.3.block.1.conv1.bias\", \"encoder.down.3.block.1.norm2.weight\", \"encoder.down.3.block.1.norm2.bias\", \"encoder.down.3.block.1.conv2.weight\", \"encoder.down.3.block.1.conv2.bias\", \"encoder.down.3.downsample.conv.weight\", \"encoder.down.3.downsample.conv.bias\", \"encoder.down.4.block.0.norm1.weight\", \"encoder.down.4.block.0.norm1.bias\", \"encoder.down.4.block.0.conv1.weight\", \"encoder.down.4.block.0.conv1.bias\", \"encoder.down.4.block.0.norm2.weight\", \"encoder.down.4.block.0.norm2.bias\", \"encoder.down.4.block.0.conv2.weight\", \"encoder.down.4.block.0.conv2.bias\", \"encoder.down.4.block.0.nin_shortcut.weight\", \"encoder.down.4.block.0.nin_shortcut.bias\", \"encoder.down.4.block.1.norm1.weight\", \"encoder.down.4.block.1.norm1.bias\", \"encoder.down.4.block.1.conv1.weight\", \"encoder.down.4.block.1.conv1.bias\", \"encoder.down.4.block.1.norm2.weight\", \"encoder.down.4.block.1.norm2.bias\", \"encoder.down.4.block.1.conv2.weight\", \"encoder.down.4.block.1.conv2.bias\", \"encoder.down.4.attn.0.norm.weight\", \"encoder.down.4.attn.0.norm.bias\", \"encoder.down.4.attn.0.qkv.weight\", \"encoder.down.4.attn.0.qkv.bias\", \"encoder.down.4.attn.0.proj_out.weight\", \"encoder.down.4.attn.0.proj_out.bias\", \"encoder.down.4.attn.1.norm.weight\", \"encoder.down.4.attn.1.norm.bias\", \"encoder.down.4.attn.1.qkv.weight\", \"encoder.down.4.attn.1.qkv.bias\", \"encoder.down.4.attn.1.proj_out.weight\", \"encoder.down.4.attn.1.proj_out.bias\", \"encoder.down.2.downsample.conv.weight\", \"encoder.down.2.downsample.conv.bias\", \"decoder.up.3.block.0.norm1.weight\", \"decoder.up.3.block.0.norm1.bias\", \"decoder.up.3.block.0.conv1.weight\", \"decoder.up.3.block.0.conv1.bias\", \"decoder.up.3.block.0.norm2.weight\", \"decoder.up.3.block.0.norm2.bias\", \"decoder.up.3.block.0.conv2.weight\", \"decoder.up.3.block.0.conv2.bias\", \"decoder.up.3.block.0.nin_shortcut.weight\", \"decoder.up.3.block.0.nin_shortcut.bias\", \"decoder.up.3.block.1.norm1.weight\", \"decoder.up.3.block.1.norm1.bias\", \"decoder.up.3.block.1.conv1.weight\", \"decoder.up.3.block.1.conv1.bias\", \"decoder.up.3.block.1.norm2.weight\", \"decoder.up.3.block.1.norm2.bias\", \"decoder.up.3.block.1.conv2.weight\", \"decoder.up.3.block.1.conv2.bias\", \"decoder.up.3.block.2.norm1.weight\", \"decoder.up.3.block.2.norm1.bias\", \"decoder.up.3.block.2.conv1.weight\", \"decoder.up.3.block.2.conv1.bias\", \"decoder.up.3.block.2.norm2.weight\", \"decoder.up.3.block.2.norm2.bias\", \"decoder.up.3.block.2.conv2.weight\", \"decoder.up.3.block.2.conv2.bias\", \"decoder.up.3.upsample.conv.weight\", \"decoder.up.3.upsample.conv.bias\", \"decoder.up.4.block.0.norm1.weight\", \"decoder.up.4.block.0.norm1.bias\", \"decoder.up.4.block.0.conv1.weight\", \"decoder.up.4.block.0.conv1.bias\", \"decoder.up.4.block.0.norm2.weight\", \"decoder.up.4.block.0.norm2.bias\", \"decoder.up.4.block.0.conv2.weight\", \"decoder.up.4.block.0.conv2.bias\", \"decoder.up.4.block.1.norm1.weight\", \"decoder.up.4.block.1.norm1.bias\", \"decoder.up.4.block.1.conv1.weight\", \"decoder.up.4.block.1.conv1.bias\", \"decoder.up.4.block.1.norm2.weight\", \"decoder.up.4.block.1.norm2.bias\", \"decoder.up.4.block.1.conv2.weight\", \"decoder.up.4.block.1.conv2.bias\", \"decoder.up.4.block.2.norm1.weight\", \"decoder.up.4.block.2.norm1.bias\", \"decoder.up.4.block.2.conv1.weight\", \"decoder.up.4.block.2.conv1.bias\", \"decoder.up.4.block.2.norm2.weight\", \"decoder.up.4.block.2.norm2.bias\", \"decoder.up.4.block.2.conv2.weight\", \"decoder.up.4.block.2.conv2.bias\", \"decoder.up.4.attn.0.norm.weight\", \"decoder.up.4.attn.0.norm.bias\", \"decoder.up.4.attn.0.qkv.weight\", \"decoder.up.4.attn.0.qkv.bias\", \"decoder.up.4.attn.0.proj_out.weight\", \"decoder.up.4.attn.0.proj_out.bias\", \"decoder.up.4.attn.1.norm.weight\", \"decoder.up.4.attn.1.norm.bias\", \"decoder.up.4.attn.1.qkv.weight\", \"decoder.up.4.attn.1.qkv.bias\", \"decoder.up.4.attn.1.proj_out.weight\", \"decoder.up.4.attn.1.proj_out.bias\", \"decoder.up.4.attn.2.norm.weight\", \"decoder.up.4.attn.2.norm.bias\", \"decoder.up.4.attn.2.qkv.weight\", \"decoder.up.4.attn.2.qkv.bias\", \"decoder.up.4.attn.2.proj_out.weight\", \"decoder.up.4.attn.2.proj_out.bias\", \"decoder.up.4.upsample.conv.weight\", \"decoder.up.4.upsample.conv.bias\". \n\tsize mismatch for encoder.down.1.block.0.conv1.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 160, 3, 3]).\n\tsize mismatch for encoder.down.1.block.0.conv1.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.1.block.0.norm2.weight: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.1.block.0.norm2.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.1.block.0.conv2.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).\n\tsize mismatch for encoder.down.1.block.0.conv2.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.1.block.1.norm1.weight: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.1.block.1.norm1.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.1.block.1.conv1.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).\n\tsize mismatch for encoder.down.1.block.1.conv1.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.1.block.1.norm2.weight: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.1.block.1.norm2.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.1.block.1.conv2.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).\n\tsize mismatch for encoder.down.1.block.1.conv2.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.1.downsample.conv.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).\n\tsize mismatch for encoder.down.1.downsample.conv.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.2.block.0.norm1.weight: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.2.block.0.norm1.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.2.block.0.conv1.weight: copying a param with shape torch.Size([320, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 320, 3, 3]).\n\tsize mismatch for encoder.down.2.block.0.conv1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for encoder.down.2.block.0.norm2.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for encoder.down.2.block.0.norm2.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for encoder.down.2.block.0.conv2.weight: copying a param with shape torch.Size([320, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).\n\tsize mismatch for encoder.down.2.block.0.conv2.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for encoder.down.2.block.0.nin_shortcut.weight: copying a param with shape torch.Size([320, 160, 1, 1]) from checkpoint, the shape in current model is torch.Size([640, 320, 1, 1]).\n\tsize mismatch for encoder.down.2.block.0.nin_shortcut.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for encoder.down.2.block.1.norm1.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for encoder.down.2.block.1.norm1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for encoder.down.2.block.1.conv1.weight: copying a param with shape torch.Size([320, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).\n\tsize mismatch for encoder.down.2.block.1.conv1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for encoder.down.2.block.1.norm2.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for encoder.down.2.block.1.norm2.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for encoder.down.2.block.1.conv2.weight: copying a param with shape torch.Size([320, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).\n\tsize mismatch for encoder.down.2.block.1.conv2.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.0.block.0.norm1.weight: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.0.block.0.norm1.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.0.block.0.conv1.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 320, 3, 3]).\n\tsize mismatch for decoder.up.1.block.0.norm1.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.1.block.0.norm1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.1.block.0.conv1.weight: copying a param with shape torch.Size([160, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 640, 3, 3]).\n\tsize mismatch for decoder.up.1.block.0.conv1.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.0.norm2.weight: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.0.norm2.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.0.conv2.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).\n\tsize mismatch for decoder.up.1.block.0.conv2.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.0.nin_shortcut.weight: copying a param with shape torch.Size([160, 320, 1, 1]) from checkpoint, the shape in current model is torch.Size([320, 640, 1, 1]).\n\tsize mismatch for decoder.up.1.block.0.nin_shortcut.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.1.norm1.weight: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.1.norm1.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.1.conv1.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).\n\tsize mismatch for decoder.up.1.block.1.conv1.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.1.norm2.weight: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.1.norm2.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.1.conv2.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).\n\tsize mismatch for decoder.up.1.block.1.conv2.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.2.norm1.weight: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.2.norm1.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.2.conv1.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).\n\tsize mismatch for decoder.up.1.block.2.conv1.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.2.norm2.weight: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.2.norm2.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.2.conv2.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).\n\tsize mismatch for decoder.up.1.block.2.conv2.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.upsample.conv.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).\n\tsize mismatch for decoder.up.1.upsample.conv.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.2.block.0.norm1.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.0.norm1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.0.conv1.weight: copying a param with shape torch.Size([320, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).\n\tsize mismatch for decoder.up.2.block.0.conv1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.0.norm2.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.0.norm2.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.0.conv2.weight: copying a param with shape torch.Size([320, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).\n\tsize mismatch for decoder.up.2.block.0.conv2.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.1.norm1.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.1.norm1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.1.conv1.weight: copying a param with shape torch.Size([320, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).\n\tsize mismatch for decoder.up.2.block.1.conv1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.1.norm2.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.1.norm2.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.1.conv2.weight: copying a param with shape torch.Size([320, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).\n\tsize mismatch for decoder.up.2.block.1.conv2.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.2.norm1.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.2.norm1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.2.conv1.weight: copying a param with shape torch.Size([320, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).\n\tsize mismatch for decoder.up.2.block.2.conv1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.2.norm2.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.2.norm2.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.2.conv2.weight: copying a param with shape torch.Size([320, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).\n\tsize mismatch for decoder.up.2.block.2.conv2.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.upsample.conv.weight: copying a param with shape torch.Size([320, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).\n\tsize mismatch for decoder.up.2.upsample.conv.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m osp\u001B[38;5;241m.\u001B[39mexists(vae_ckpt): os\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwget \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhf_home\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvae_ckpt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# load checkpoints\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m \u001B[43mvae_local\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvae_ckpt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstrict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Codes/Uni/COMP4528/mini-project/SinVAR/vqvae/vqvae.py:109\u001B[0m, in \u001B[0;36mVQVAE.load_state_dict\u001B[0;34m(self, state_dict, strict, assign)\u001B[0m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mquantize.ema_vocab_hit_SV\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m state_dict \u001B[38;5;129;01mand\u001B[39;00m state_dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mquantize.ema_vocab_hit_SV\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m \\\n\u001B[1;32m    107\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquantize\u001B[38;5;241m.\u001B[39mema_vocab_hit_SV\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]:\n\u001B[1;32m    108\u001B[0m     state_dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mquantize.ema_vocab_hit_SV\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquantize\u001B[38;5;241m.\u001B[39mema_vocab_hit_SV\n\u001B[0;32m--> 109\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstrict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstrict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43massign\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43massign\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/comp4528/lib/python3.12/site-packages/torch/nn/modules/module.py:2581\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[0;34m(self, state_dict, strict, assign)\u001B[0m\n\u001B[1;32m   2573\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[1;32m   2574\u001B[0m             \u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m   2575\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2576\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)\n\u001B[1;32m   2577\u001B[0m             ),\n\u001B[1;32m   2578\u001B[0m         )\n\u001B[1;32m   2580\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 2581\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   2582\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2583\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)\n\u001B[1;32m   2584\u001B[0m         )\n\u001B[1;32m   2585\u001B[0m     )\n\u001B[1;32m   2586\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for VQVAE:\n\tMissing key(s) in state_dict: \"encoder.down.1.block.0.nin_shortcut.weight\", \"encoder.down.1.block.0.nin_shortcut.bias\", \"encoder.down.2.attn.0.norm.weight\", \"encoder.down.2.attn.0.norm.bias\", \"encoder.down.2.attn.0.qkv.weight\", \"encoder.down.2.attn.0.qkv.bias\", \"encoder.down.2.attn.0.proj_out.weight\", \"encoder.down.2.attn.0.proj_out.bias\", \"encoder.down.2.attn.1.norm.weight\", \"encoder.down.2.attn.1.norm.bias\", \"encoder.down.2.attn.1.qkv.weight\", \"encoder.down.2.attn.1.qkv.bias\", \"encoder.down.2.attn.1.proj_out.weight\", \"encoder.down.2.attn.1.proj_out.bias\", \"decoder.up.0.block.0.nin_shortcut.weight\", \"decoder.up.0.block.0.nin_shortcut.bias\", \"decoder.up.2.attn.0.norm.weight\", \"decoder.up.2.attn.0.norm.bias\", \"decoder.up.2.attn.0.qkv.weight\", \"decoder.up.2.attn.0.qkv.bias\", \"decoder.up.2.attn.0.proj_out.weight\", \"decoder.up.2.attn.0.proj_out.bias\", \"decoder.up.2.attn.1.norm.weight\", \"decoder.up.2.attn.1.norm.bias\", \"decoder.up.2.attn.1.qkv.weight\", \"decoder.up.2.attn.1.qkv.bias\", \"decoder.up.2.attn.1.proj_out.weight\", \"decoder.up.2.attn.1.proj_out.bias\", \"decoder.up.2.attn.2.norm.weight\", \"decoder.up.2.attn.2.norm.bias\", \"decoder.up.2.attn.2.qkv.weight\", \"decoder.up.2.attn.2.qkv.bias\", \"decoder.up.2.attn.2.proj_out.weight\", \"decoder.up.2.attn.2.proj_out.bias\". \n\tUnexpected key(s) in state_dict: \"encoder.down.3.block.0.norm1.weight\", \"encoder.down.3.block.0.norm1.bias\", \"encoder.down.3.block.0.conv1.weight\", \"encoder.down.3.block.0.conv1.bias\", \"encoder.down.3.block.0.norm2.weight\", \"encoder.down.3.block.0.norm2.bias\", \"encoder.down.3.block.0.conv2.weight\", \"encoder.down.3.block.0.conv2.bias\", \"encoder.down.3.block.1.norm1.weight\", \"encoder.down.3.block.1.norm1.bias\", \"encoder.down.3.block.1.conv1.weight\", \"encoder.down.3.block.1.conv1.bias\", \"encoder.down.3.block.1.norm2.weight\", \"encoder.down.3.block.1.norm2.bias\", \"encoder.down.3.block.1.conv2.weight\", \"encoder.down.3.block.1.conv2.bias\", \"encoder.down.3.downsample.conv.weight\", \"encoder.down.3.downsample.conv.bias\", \"encoder.down.4.block.0.norm1.weight\", \"encoder.down.4.block.0.norm1.bias\", \"encoder.down.4.block.0.conv1.weight\", \"encoder.down.4.block.0.conv1.bias\", \"encoder.down.4.block.0.norm2.weight\", \"encoder.down.4.block.0.norm2.bias\", \"encoder.down.4.block.0.conv2.weight\", \"encoder.down.4.block.0.conv2.bias\", \"encoder.down.4.block.0.nin_shortcut.weight\", \"encoder.down.4.block.0.nin_shortcut.bias\", \"encoder.down.4.block.1.norm1.weight\", \"encoder.down.4.block.1.norm1.bias\", \"encoder.down.4.block.1.conv1.weight\", \"encoder.down.4.block.1.conv1.bias\", \"encoder.down.4.block.1.norm2.weight\", \"encoder.down.4.block.1.norm2.bias\", \"encoder.down.4.block.1.conv2.weight\", \"encoder.down.4.block.1.conv2.bias\", \"encoder.down.4.attn.0.norm.weight\", \"encoder.down.4.attn.0.norm.bias\", \"encoder.down.4.attn.0.qkv.weight\", \"encoder.down.4.attn.0.qkv.bias\", \"encoder.down.4.attn.0.proj_out.weight\", \"encoder.down.4.attn.0.proj_out.bias\", \"encoder.down.4.attn.1.norm.weight\", \"encoder.down.4.attn.1.norm.bias\", \"encoder.down.4.attn.1.qkv.weight\", \"encoder.down.4.attn.1.qkv.bias\", \"encoder.down.4.attn.1.proj_out.weight\", \"encoder.down.4.attn.1.proj_out.bias\", \"encoder.down.2.downsample.conv.weight\", \"encoder.down.2.downsample.conv.bias\", \"decoder.up.3.block.0.norm1.weight\", \"decoder.up.3.block.0.norm1.bias\", \"decoder.up.3.block.0.conv1.weight\", \"decoder.up.3.block.0.conv1.bias\", \"decoder.up.3.block.0.norm2.weight\", \"decoder.up.3.block.0.norm2.bias\", \"decoder.up.3.block.0.conv2.weight\", \"decoder.up.3.block.0.conv2.bias\", \"decoder.up.3.block.0.nin_shortcut.weight\", \"decoder.up.3.block.0.nin_shortcut.bias\", \"decoder.up.3.block.1.norm1.weight\", \"decoder.up.3.block.1.norm1.bias\", \"decoder.up.3.block.1.conv1.weight\", \"decoder.up.3.block.1.conv1.bias\", \"decoder.up.3.block.1.norm2.weight\", \"decoder.up.3.block.1.norm2.bias\", \"decoder.up.3.block.1.conv2.weight\", \"decoder.up.3.block.1.conv2.bias\", \"decoder.up.3.block.2.norm1.weight\", \"decoder.up.3.block.2.norm1.bias\", \"decoder.up.3.block.2.conv1.weight\", \"decoder.up.3.block.2.conv1.bias\", \"decoder.up.3.block.2.norm2.weight\", \"decoder.up.3.block.2.norm2.bias\", \"decoder.up.3.block.2.conv2.weight\", \"decoder.up.3.block.2.conv2.bias\", \"decoder.up.3.upsample.conv.weight\", \"decoder.up.3.upsample.conv.bias\", \"decoder.up.4.block.0.norm1.weight\", \"decoder.up.4.block.0.norm1.bias\", \"decoder.up.4.block.0.conv1.weight\", \"decoder.up.4.block.0.conv1.bias\", \"decoder.up.4.block.0.norm2.weight\", \"decoder.up.4.block.0.norm2.bias\", \"decoder.up.4.block.0.conv2.weight\", \"decoder.up.4.block.0.conv2.bias\", \"decoder.up.4.block.1.norm1.weight\", \"decoder.up.4.block.1.norm1.bias\", \"decoder.up.4.block.1.conv1.weight\", \"decoder.up.4.block.1.conv1.bias\", \"decoder.up.4.block.1.norm2.weight\", \"decoder.up.4.block.1.norm2.bias\", \"decoder.up.4.block.1.conv2.weight\", \"decoder.up.4.block.1.conv2.bias\", \"decoder.up.4.block.2.norm1.weight\", \"decoder.up.4.block.2.norm1.bias\", \"decoder.up.4.block.2.conv1.weight\", \"decoder.up.4.block.2.conv1.bias\", \"decoder.up.4.block.2.norm2.weight\", \"decoder.up.4.block.2.norm2.bias\", \"decoder.up.4.block.2.conv2.weight\", \"decoder.up.4.block.2.conv2.bias\", \"decoder.up.4.attn.0.norm.weight\", \"decoder.up.4.attn.0.norm.bias\", \"decoder.up.4.attn.0.qkv.weight\", \"decoder.up.4.attn.0.qkv.bias\", \"decoder.up.4.attn.0.proj_out.weight\", \"decoder.up.4.attn.0.proj_out.bias\", \"decoder.up.4.attn.1.norm.weight\", \"decoder.up.4.attn.1.norm.bias\", \"decoder.up.4.attn.1.qkv.weight\", \"decoder.up.4.attn.1.qkv.bias\", \"decoder.up.4.attn.1.proj_out.weight\", \"decoder.up.4.attn.1.proj_out.bias\", \"decoder.up.4.attn.2.norm.weight\", \"decoder.up.4.attn.2.norm.bias\", \"decoder.up.4.attn.2.qkv.weight\", \"decoder.up.4.attn.2.qkv.bias\", \"decoder.up.4.attn.2.proj_out.weight\", \"decoder.up.4.attn.2.proj_out.bias\", \"decoder.up.4.upsample.conv.weight\", \"decoder.up.4.upsample.conv.bias\". \n\tsize mismatch for encoder.down.1.block.0.conv1.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 160, 3, 3]).\n\tsize mismatch for encoder.down.1.block.0.conv1.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.1.block.0.norm2.weight: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.1.block.0.norm2.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.1.block.0.conv2.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).\n\tsize mismatch for encoder.down.1.block.0.conv2.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.1.block.1.norm1.weight: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.1.block.1.norm1.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.1.block.1.conv1.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).\n\tsize mismatch for encoder.down.1.block.1.conv1.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.1.block.1.norm2.weight: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.1.block.1.norm2.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.1.block.1.conv2.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).\n\tsize mismatch for encoder.down.1.block.1.conv2.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.1.downsample.conv.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).\n\tsize mismatch for encoder.down.1.downsample.conv.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.2.block.0.norm1.weight: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.2.block.0.norm1.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for encoder.down.2.block.0.conv1.weight: copying a param with shape torch.Size([320, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 320, 3, 3]).\n\tsize mismatch for encoder.down.2.block.0.conv1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for encoder.down.2.block.0.norm2.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for encoder.down.2.block.0.norm2.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for encoder.down.2.block.0.conv2.weight: copying a param with shape torch.Size([320, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).\n\tsize mismatch for encoder.down.2.block.0.conv2.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for encoder.down.2.block.0.nin_shortcut.weight: copying a param with shape torch.Size([320, 160, 1, 1]) from checkpoint, the shape in current model is torch.Size([640, 320, 1, 1]).\n\tsize mismatch for encoder.down.2.block.0.nin_shortcut.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for encoder.down.2.block.1.norm1.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for encoder.down.2.block.1.norm1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for encoder.down.2.block.1.conv1.weight: copying a param with shape torch.Size([320, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).\n\tsize mismatch for encoder.down.2.block.1.conv1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for encoder.down.2.block.1.norm2.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for encoder.down.2.block.1.norm2.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for encoder.down.2.block.1.conv2.weight: copying a param with shape torch.Size([320, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).\n\tsize mismatch for encoder.down.2.block.1.conv2.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.0.block.0.norm1.weight: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.0.block.0.norm1.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.0.block.0.conv1.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 320, 3, 3]).\n\tsize mismatch for decoder.up.1.block.0.norm1.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.1.block.0.norm1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.1.block.0.conv1.weight: copying a param with shape torch.Size([160, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 640, 3, 3]).\n\tsize mismatch for decoder.up.1.block.0.conv1.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.0.norm2.weight: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.0.norm2.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.0.conv2.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).\n\tsize mismatch for decoder.up.1.block.0.conv2.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.0.nin_shortcut.weight: copying a param with shape torch.Size([160, 320, 1, 1]) from checkpoint, the shape in current model is torch.Size([320, 640, 1, 1]).\n\tsize mismatch for decoder.up.1.block.0.nin_shortcut.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.1.norm1.weight: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.1.norm1.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.1.conv1.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).\n\tsize mismatch for decoder.up.1.block.1.conv1.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.1.norm2.weight: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.1.norm2.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.1.conv2.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).\n\tsize mismatch for decoder.up.1.block.1.conv2.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.2.norm1.weight: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.2.norm1.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.2.conv1.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).\n\tsize mismatch for decoder.up.1.block.2.conv1.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.2.norm2.weight: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.2.norm2.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.block.2.conv2.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).\n\tsize mismatch for decoder.up.1.block.2.conv2.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.1.upsample.conv.weight: copying a param with shape torch.Size([160, 160, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).\n\tsize mismatch for decoder.up.1.upsample.conv.bias: copying a param with shape torch.Size([160]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for decoder.up.2.block.0.norm1.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.0.norm1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.0.conv1.weight: copying a param with shape torch.Size([320, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).\n\tsize mismatch for decoder.up.2.block.0.conv1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.0.norm2.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.0.norm2.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.0.conv2.weight: copying a param with shape torch.Size([320, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).\n\tsize mismatch for decoder.up.2.block.0.conv2.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.1.norm1.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.1.norm1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.1.conv1.weight: copying a param with shape torch.Size([320, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).\n\tsize mismatch for decoder.up.2.block.1.conv1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.1.norm2.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.1.norm2.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.1.conv2.weight: copying a param with shape torch.Size([320, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).\n\tsize mismatch for decoder.up.2.block.1.conv2.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.2.norm1.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.2.norm1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.2.conv1.weight: copying a param with shape torch.Size([320, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).\n\tsize mismatch for decoder.up.2.block.2.conv1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.2.norm2.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.2.norm2.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.block.2.conv2.weight: copying a param with shape torch.Size([320, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).\n\tsize mismatch for decoder.up.2.block.2.conv2.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640]).\n\tsize mismatch for decoder.up.2.upsample.conv.weight: copying a param with shape torch.Size([320, 320, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).\n\tsize mismatch for decoder.up.2.upsample.conv.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([640])."
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T08:28:28.319297Z",
     "start_time": "2025-04-20T08:28:28.319239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_p = lambda m: f'{sum(p.numel() for p in m.parameters())/1e6:.2f}'\n",
    "print(f'[INIT][#para] ' + ', '.join([f'{k}={count_p(m)}' for k, m in (('VAE', vae_local), ('VAE.enc', vae_local.encoder), ('VAE.dec', vae_local.decoder), ('VAE.quant', vae_local.quantize))]))\n",
    "print(f'[INIT][#para] ' + ', '.join([f'{k}={count_p(m)}' for k, m in (('VAR', var_wo_ddp),)]) + '\\n\\n')"
   ],
   "id": "fca2e5ad35304b93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Move it to mps for quicker inference\n",
    "vae_local.to(torch.device('mps'))\n",
    "\n",
    "device = next(vae_local.parameters()).device\n",
    "print(f\"Current Device: {device}\")"
   ],
   "id": "16b1c951bb1e09b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "\n",
    "In order to either test the loaded model or to train the var, we need to first have a dataset."
   ],
   "id": "3680c794a9ce5cca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### `Dataset` Object\n",
    "\n",
    "This is the dataset object. Note that this dataset only has one image."
   ],
   "id": "a9bbcc3027fcdefd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "image: np.ndarray = io.imread('./data/jcsmr.jpg')\n",
    "height, width = image.shape[:2]\n",
    "channel = image.shape[2] if image.ndim == 3 else 1\n",
    "dtype = image.dtype\n",
    "\n",
    "print(f\"Image dimension: {height=}, {width=}, {channel=}\")\n",
    "print(f\"Image dtype: {dtype=}\")\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title(f\"{width}×{height}, {channel} channels\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "id": "b23fcb9a6ba3c30d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "class SingleImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    The single image dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, image_path, transform, seed, train=True):\n",
    "        \"\"\"\n",
    "        This is the single Image dataset. Note that the single image dataset contains only one image. Therefore, it is important note that it is important to use random cropping and other\n",
    "        transforming methods to augment the dataset.\n",
    "        \n",
    "        :param image_path: This is the path of the data.\n",
    "        :param transform: Normally, this is optional. However, it is required for this dataset given that it has only one image. Also, please use some corpping.\n",
    "        \"\"\"\n",
    "        self.image_path = image_path\n",
    "        self.image = io.imread(image_path)\n",
    "        self.transform = transform\n",
    "        self.seed = seed\n",
    "        self.train = train\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 10000 if self.train else 4285\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        if isinstance(idx, list):\n",
    "            return [self.__getitem__(i) for i in idx]\n",
    "        \n",
    "        if not isinstance(idx, int):\n",
    "            raise TypeError(f\"`idx` must be an integer or a list but got {type(idx)}\")\n",
    "        \n",
    "        # Set deterministic augmentation for given idx\n",
    "        torch.manual_seed(self.seed + idx) \n",
    "        return self.transform(self.image)    "
   ],
   "id": "fdf0e38c87629952",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### `Transform`\n",
    "\n",
    "Then, we need to define the transform to use."
   ],
   "id": "bdafb5ffefc11955"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "preview_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.RandomResizedCrop(size=256, scale=(0.08, 0.5), ratio=(0.75, 2.0)),\n",
    "    # v2.RandomPerspective(distortion_scale=0.3, p=0.5),    # This transformation causes unwanted padding, so I ended up disabling\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    # v2.RandomRotation(degrees=5),                         # This transformation causes unwanted padding, so I ended up disabling\n",
    "    v2.ColorJitter(brightness=0.2, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "])\n",
    "\n",
    "# Generate and display augmented images\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i in range(4):\n",
    "    aug = preview_transform(image)\n",
    "    img_pil = to_pil_image(aug)\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.imshow(img_pil)\n",
    "    plt.title(f'Augmented #{i+1}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "1adf86100df62ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_path = './data/jcsmr.jpg'\n",
    "train_transform = v2.Compose([\n",
    "    preview_transform,\n",
    "    v2.Normalize(mean=[0.5]*3, std=[0.5]*3),\n",
    "])\n",
    "train_dataset = SingleImageDataset(image_path, transform=train_transform, seed=42)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "val_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.RandomResizedCrop(size=256),\n",
    "    v2.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "val_dataset = SingleImageDataset(image_path, transform=val_transform, seed=1234567891, train=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "test_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.RandomResizedCrop(size=256),\n",
    "])\n",
    "test_dataset = SingleImageDataset(image_path, transform=test_transform, seed=9876543210, train=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ],
   "id": "595f3cc947d61531",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## VQVAE Validation\n",
    "\n",
    "First, I want to make sure that the VQVAE is reconstructing correctly."
   ],
   "id": "d570f7107ef3c6fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchvision.transforms.functional import normalize\n",
    "def denorm(x: torch.Tensor):\n",
    "    return normalize(x, mean=[-1]*3, std=[1/0.5]*3)\n",
    "    \n",
    "def show_vqvae_reconstruction(vae: VQVAE, dataloader, device, num_images=4):\n",
    "    vae.eval()\n",
    "    batch: torch.Tensor = next(iter(dataloader))[:num_images].to('cpu')\n",
    "    # print(batch.shape)    # [B, C, H, W] as expected\n",
    "    \n",
    "    vae_cpu = vae_local.to('cpu').eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        recon = vae_cpu.img_to_reconstructed_img(x=batch, last_one=True)\n",
    "\n",
    "    # --- Visualization --- #\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    for i in range(batch.shape[0]):\n",
    "        # Original\n",
    "        plt.subplot(2, batch.shape[0], i + 1)\n",
    "        plt.imshow(to_pil_image((batch[i].cpu())))\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Reconstructed\n",
    "        plt.subplot(2, batch.shape[0], i + 1 + batch.shape[0])\n",
    "        plt.imshow(to_pil_image((recon[i].cpu())))\n",
    "        plt.title(\"Reconstructed\")\n",
    "        plt.axis(\"off\")    \n",
    "\n",
    "print(len(test_dataset)) \n",
    "show_vqvae_reconstruction(vae_local, test_loader, vae_var_config['device'], num_images=4) "
   ],
   "id": "222006aedad75b82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# URL of an ImageNet sample image (Labrador Retriever)\n",
    "img_url = \"https://raw.githubusercontent.com/pytorch/hub/master/images/dog.jpg\"\n",
    "filename = \"imagenet_sample.jpg\"  # Save as this filename\n",
    "\n",
    "# Download and save\n",
    "response = requests.get(img_url)\n",
    "img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "img.save(filename)\n",
    "\n",
    "print(f\"✅ Image downloaded and saved as '{filename}'\")\n",
    "\n",
    "plt.imshow(img)"
   ],
   "id": "c642b53e93735a6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "img_tensor: torch.Tensor = test_transform(img).unsqueeze(0).to(\"cpu\")  # [1, 3, 256, 256]\n",
    "img_tensor.shape, img_tensor.device"
   ],
   "id": "d388b9c03d2a0c4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vae_cpu = vae_local.to('cpu').eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    recon = vae_cpu.img_to_reconstructed_img(x=img_tensor, last_one=True)\n",
    "\n",
    "# --- Visualization --- #\n",
    "plt.figure(figsize=(12, 4))\n",
    "# Original\n",
    "plt.imshow(to_pil_image((img_tensor[0].cpu())))\n",
    "plt.title(\"Original\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Reconstructed\n",
    "plt.imshow(to_pil_image((recon[0].cpu())))\n",
    "plt.title(\"Reconstructed\")\n",
    "plt.axis(\"off\")    "
   ],
   "id": "8197578655b255ee",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
